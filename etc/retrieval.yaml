epochs: 5
batch_size: 4
max_seq_length: 512
pooling: 'cls'
dropout: 0.1
margin: 0.3
scale: 30
output_emb_size: 256
learning_rate: 1e-5
warmup_proportion: 0.1
bertadam: False
threshold: 0.5
hnsw_max_elements: 1000000
hnsw_ef: 100
hnsw_m: 100

best_model: 'retrieval_match_classification_best'
data_path: '/media/mesie/F0E66F06E66ECC82/数据/paddle/baike_qa_multilabel'
model_path: '/home/mesie/python/aia-nlp-service/lib/pretrained/albert_chinese_base'
#model_path: '/media/mesie/F0E66F06E66ECC82/model/nlp_structbert_sentence-similarity_chinese-base'